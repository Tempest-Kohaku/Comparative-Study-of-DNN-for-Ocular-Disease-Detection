{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d9b12eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Images: 100%|██████████████████████████████████████████████████████| 8000/8000 [00:40<00:00, 196.01img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All preprocessed images saved in: C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODIR-5K\\Preprocessed_Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating file paths: 100%|█████████████████████████████████████████████████████| 6392/6392 [00:00<00:00, 145267.51it/s]\n",
      "Updating file paths: 100%|█████████████████████████████████████████████████████| 6392/6392 [00:00<00:00, 220413.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New Excel file saved at: C:\\Users\\Manas\\IoT Research Models\\full_df.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Define paths\n",
    "base_folder = r\"C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODIR-5K\"\n",
    "training_folder = os.path.join(base_folder, \"Training Images\")\n",
    "testing_folder = os.path.join(base_folder, \"Testing Images\")\n",
    "preprocessed_folder = os.path.join(base_folder, \"Preprocessed_Images\")  # New folder\n",
    "csv_path = r\"C:\\Users\\Manas\\IoT Research Models\\full_df.csv\"  # Input Excel file\n",
    "output_xlsx_path = r\"C:\\Users\\Manas\\IoT Research Models\\full_df.xlsx\"  # Output Excel file\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(preprocessed_folder, exist_ok=True)\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        filename = os.path.basename(image_path)  # Get image filename\n",
    "        save_path = os.path.join(preprocessed_folder, filename)  # New destination\n",
    "\n",
    "        img = cv2.imread(image_path)  # Read image\n",
    "        if img is None:\n",
    "            print(f\"⚠️ Warning: Could not load {image_path}\")\n",
    "            return filename, None  # Skip if image is corrupted\n",
    "\n",
    "        img = cv2.resize(img, (224, 224))  # Resize\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        img = img / 255.0  # Normalize\n",
    "\n",
    "        # Convert to uint8 (optional for saving)\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "        # Save processed image\n",
    "        cv2.imwrite(save_path, img)\n",
    "\n",
    "        return filename, save_path  # Return new path\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return filename, None\n",
    "\n",
    "# Find all images in both Training and Testing folders\n",
    "training_images = glob.glob(os.path.join(training_folder, \"*.jpg\")) + glob.glob(os.path.join(training_folder, \"*.png\"))\n",
    "testing_images = glob.glob(os.path.join(testing_folder, \"*.jpg\")) + glob.glob(os.path.join(testing_folder, \"*.png\"))\n",
    "all_images = training_images + testing_images  # Merge lists\n",
    "\n",
    "# Process images in parallel with tqdm\n",
    "new_image_paths = {}\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:  # Use 8 threads\n",
    "    futures = {executor.submit(preprocess_image, img): img for img in all_images}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Preprocessing Images\", unit=\"img\"):\n",
    "        filename, result = future.result()\n",
    "        if result:\n",
    "            new_image_paths[filename] = result  # Store new paths\n",
    "\n",
    "print(f\"✅ All preprocessed images saved in: {preprocessed_folder}\")\n",
    "\n",
    "# Load Excel file and update file paths\n",
    "original_df = pd.read_csv(csv_path)  # Read Excel file\n",
    "\n",
    "# Update file paths in the dataset\n",
    "tqdm.pandas(desc=\"Updating file paths\")\n",
    "original_df[\"Left-Fundus\"] = original_df[\"Left-Fundus\"].progress_apply(lambda x: new_image_paths.get(os.path.basename(x), None) if isinstance(x, str) else x)\n",
    "original_df[\"Right-Fundus\"] = original_df[\"Right-Fundus\"].progress_apply(lambda x: new_image_paths.get(os.path.basename(x), None) if isinstance(x, str) else x)\n",
    "\n",
    "# Drop any rows where images are missing after preprocessing\n",
    "original_df.dropna(subset=[\"Left-Fundus\", \"Right-Fundus\"], inplace=True)\n",
    "\n",
    "# Save updated dataset as an Excel file\n",
    "original_df.to_excel(output_xlsx_path, index=False)\n",
    "print(f\"✅ New Excel file saved at: {output_xlsx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a2a40bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Sex</th>\n",
       "      <th>Left-Fundus</th>\n",
       "      <th>Right-Fundus</th>\n",
       "      <th>Left-Diagnostic Keywords</th>\n",
       "      <th>Right-Diagnostic Keywords</th>\n",
       "      <th>N</th>\n",
       "      <th>D</th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "      <th>H</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>labels</th>\n",
       "      <th>target</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>Female</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>cataract</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['N']</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0_right.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>Male</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['N']</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1_right.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>Male</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>laser spot，moderate non proliferative retinopathy</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['D']</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2_right.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>Male</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>macular epiretinal membrane</td>\n",
       "      <td>mild nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['D']</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>4_right.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>Female</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['D']</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>5_right.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Patient Age Patient Sex  \\\n",
       "0   0           69      Female   \n",
       "1   1           57        Male   \n",
       "2   2           42        Male   \n",
       "3   4           53        Male   \n",
       "4   5           50      Female   \n",
       "\n",
       "                                         Left-Fundus  \\\n",
       "0  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "1  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "2  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "3  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "4  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "\n",
       "                                        Right-Fundus  \\\n",
       "0  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "1  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "2  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "3  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "4  C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODI...   \n",
       "\n",
       "                            Left-Diagnostic Keywords  \\\n",
       "0                                           cataract   \n",
       "1                                      normal fundus   \n",
       "2  laser spot，moderate non proliferative retinopathy   \n",
       "3                        macular epiretinal membrane   \n",
       "4             moderate non proliferative retinopathy   \n",
       "\n",
       "                Right-Diagnostic Keywords  N  D  G  C  A  H  M  O labels  \\\n",
       "0                           normal fundus  0  0  0  1  0  0  0  0  ['N']   \n",
       "1                           normal fundus  1  0  0  0  0  0  0  0  ['N']   \n",
       "2  moderate non proliferative retinopathy  0  1  0  0  0  0  0  1  ['D']   \n",
       "3       mild nonproliferative retinopathy  0  1  0  0  0  0  0  1  ['D']   \n",
       "4  moderate non proliferative retinopathy  0  1  0  0  0  0  0  0  ['D']   \n",
       "\n",
       "                     target     filename  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0]  0_right.jpg  \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0]  1_right.jpg  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0]  2_right.jpg  \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0]  4_right.jpg  \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0]  5_right.jpg  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b368f79-e4cb-4067-a1d5-3e1171317a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = original_df.drop(\"filepath\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c735b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One batch shape: (16, 224, 224, 3)\n",
      "Time taken for one batch: 0.33202433586120605 seconds\n",
      "Starting training...\n",
      "Epoch 1/50\n",
      "640/640 [==============================] - 65s 87ms/step - loss: 1.6654 - accuracy: 0.3650 - val_loss: 1.5605 - val_accuracy: 0.4138 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 1.5777 - accuracy: 0.3897 - val_loss: 1.5099 - val_accuracy: 0.4259 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "640/640 [==============================] - 55s 85ms/step - loss: 1.5449 - accuracy: 0.3970 - val_loss: 1.4947 - val_accuracy: 0.4333 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "640/640 [==============================] - 44s 69ms/step - loss: 1.5235 - accuracy: 0.3976 - val_loss: 1.4846 - val_accuracy: 0.4345 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "640/640 [==============================] - 43s 67ms/step - loss: 1.5040 - accuracy: 0.4128 - val_loss: 1.4687 - val_accuracy: 0.4364 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "640/640 [==============================] - 45s 71ms/step - loss: 1.4841 - accuracy: 0.4109 - val_loss: 1.4669 - val_accuracy: 0.4454 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "640/640 [==============================] - 44s 69ms/step - loss: 1.4682 - accuracy: 0.4171 - val_loss: 1.4623 - val_accuracy: 0.4392 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "640/640 [==============================] - 46s 72ms/step - loss: 1.4545 - accuracy: 0.4188 - val_loss: 1.4523 - val_accuracy: 0.4368 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "640/640 [==============================] - 43s 67ms/step - loss: 1.4486 - accuracy: 0.4300 - val_loss: 1.4485 - val_accuracy: 0.4419 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "640/640 [==============================] - 45s 70ms/step - loss: 1.4274 - accuracy: 0.4339 - val_loss: 1.4446 - val_accuracy: 0.4431 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "640/640 [==============================] - 44s 69ms/step - loss: 1.4157 - accuracy: 0.4426 - val_loss: 1.4342 - val_accuracy: 0.4501 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "640/640 [==============================] - 42s 65ms/step - loss: 1.4071 - accuracy: 0.4442 - val_loss: 1.4323 - val_accuracy: 0.4497 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "640/640 [==============================] - 42s 66ms/step - loss: 1.3965 - accuracy: 0.4493 - val_loss: 1.4323 - val_accuracy: 0.4451 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "640/640 [==============================] - 41s 65ms/step - loss: 1.3880 - accuracy: 0.4500 - val_loss: 1.4273 - val_accuracy: 0.4490 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "640/640 [==============================] - 42s 65ms/step - loss: 1.3678 - accuracy: 0.4560 - val_loss: 1.4251 - val_accuracy: 0.4486 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "640/640 [==============================] - 41s 63ms/step - loss: 1.3619 - accuracy: 0.4573 - val_loss: 1.4258 - val_accuracy: 0.4458 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "640/640 [==============================] - 41s 65ms/step - loss: 1.3449 - accuracy: 0.4657 - val_loss: 1.4256 - val_accuracy: 0.4427 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "640/640 [==============================] - 41s 65ms/step - loss: 1.3340 - accuracy: 0.4658 - val_loss: 1.4228 - val_accuracy: 0.4435 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "640/640 [==============================] - 43s 66ms/step - loss: 1.3220 - accuracy: 0.4790 - val_loss: 1.4156 - val_accuracy: 0.4607 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "640/640 [==============================] - 43s 67ms/step - loss: 1.3115 - accuracy: 0.4792 - val_loss: 1.4144 - val_accuracy: 0.4501 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "640/640 [==============================] - 42s 66ms/step - loss: 1.2998 - accuracy: 0.4833 - val_loss: 1.4186 - val_accuracy: 0.4454 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "640/640 [==============================] - 42s 66ms/step - loss: 1.2866 - accuracy: 0.4886 - val_loss: 1.4219 - val_accuracy: 0.4497 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "640/640 [==============================] - 42s 65ms/step - loss: 1.2792 - accuracy: 0.4927 - val_loss: 1.4167 - val_accuracy: 0.4494 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "640/640 [==============================] - 43s 67ms/step - loss: 1.2431 - accuracy: 0.5060 - val_loss: 1.4101 - val_accuracy: 0.4548 - lr: 5.0000e-05\n",
      "Epoch 25/50\n",
      "640/640 [==============================] - 42s 65ms/step - loss: 1.2189 - accuracy: 0.5145 - val_loss: 1.4113 - val_accuracy: 0.4572 - lr: 5.0000e-05\n",
      "Epoch 26/50\n",
      "640/640 [==============================] - 41s 65ms/step - loss: 1.2110 - accuracy: 0.5153 - val_loss: 1.4122 - val_accuracy: 0.4568 - lr: 5.0000e-05\n",
      "Epoch 27/50\n",
      "640/640 [==============================] - 45s 70ms/step - loss: 1.1979 - accuracy: 0.5253 - val_loss: 1.4089 - val_accuracy: 0.4568 - lr: 5.0000e-05\n",
      "Epoch 28/50\n",
      "640/640 [==============================] - 42s 65ms/step - loss: 1.1841 - accuracy: 0.5328 - val_loss: 1.4142 - val_accuracy: 0.4537 - lr: 5.0000e-05\n",
      "Epoch 29/50\n",
      "640/640 [==============================] - 42s 66ms/step - loss: 1.1696 - accuracy: 0.5382 - val_loss: 1.4191 - val_accuracy: 0.4544 - lr: 5.0000e-05\n",
      "Epoch 30/50\n",
      "640/640 [==============================] - 41s 64ms/step - loss: 1.1689 - accuracy: 0.5330 - val_loss: 1.4151 - val_accuracy: 0.4607 - lr: 5.0000e-05\n",
      "Epoch 31/50\n",
      "640/640 [==============================] - 42s 66ms/step - loss: 1.1425 - accuracy: 0.5516 - val_loss: 1.4182 - val_accuracy: 0.4630 - lr: 2.5000e-05\n",
      "Epoch 32/50\n",
      "640/640 [==============================] - 43s 67ms/step - loss: 1.1331 - accuracy: 0.5566 - val_loss: 1.4149 - val_accuracy: 0.4595 - lr: 2.5000e-05\n",
      "✅ Model saved as 'ocular_disease_resnet50_model.h5'\n",
      "160/160 [==============================] - 9s 54ms/step - loss: 1.4089 - accuracy: 0.4568\n",
      "✅ Validation Accuracy: 45.68%\n",
      "160/160 [==============================] - 16s 73ms/step\n",
      "\n",
      "--- Metrics for Comparative Study ---\n",
      "Accuracy: 0.4568\n",
      "ROC AUC Score: 0.7582\n",
      "Precision: 0.4256\n",
      "Recall: 0.4568\n",
      "F1 Score: 0.4185\n",
      "Number of Parameters: 26222984\n",
      "Training Time: 1406.98 seconds\n",
      "Number of Layers: 8\n",
      "\n",
      "Confusion Matrix:\n",
      "[[570 225   9  17   1   0   5  21]\n",
      " [338 447   6  10   3   9  15  26]\n",
      " [ 78  35  15  10   0   0   3   6]\n",
      " [ 48  32   2  43   1   0   0   4]\n",
      " [ 51  35   0   1   0   0   3   5]\n",
      " [ 11  16   1   0   0   1   0   0]\n",
      " [ 20  21   4   2   0   1  61   2]\n",
      " [141 159   1   6   0   2   3  31]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.45      0.67      0.54       848\n",
      "           D       0.46      0.52      0.49       854\n",
      "           G       0.39      0.10      0.16       147\n",
      "           C       0.48      0.33      0.39       130\n",
      "           A       0.00      0.00      0.00        95\n",
      "           H       0.08      0.03      0.05        29\n",
      "           M       0.68      0.55      0.61       111\n",
      "           O       0.33      0.09      0.14       343\n",
      "\n",
      "    accuracy                           0.46      2557\n",
      "   macro avg       0.36      0.29      0.30      2557\n",
      "weighted avg       0.43      0.46      0.42      2557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# --------------------- Optional: Mixed Precision ---------------------\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# --------------------- Optimizations ---------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SHUFFLE_BUFFER = 250  # Increased shuffle buffer for more randomness\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Set up logging for any preprocessing errors\n",
    "logging.basicConfig(filename=\"preprocessing_errors.log\", level=logging.ERROR)\n",
    "\n",
    "# Define paths\n",
    "base_folder = r\"C:\\Users\\Manas\\IoT Research Models\\ODIR-5K\\ODIR-5K\"\n",
    "xlsx_path = r\"C:\\Users\\Manas\\IoT Research Models\\full_df.xlsx\"  # Dataset Excel file\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(xlsx_path)\n",
    "labels_list = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "df['target'] = df[labels_list].idxmax(axis=1)\n",
    "\n",
    "# Prepare image file paths and corresponding targets\n",
    "left_paths = df['Left-Fundus'].dropna().tolist()\n",
    "right_paths = df['Right-Fundus'].dropna().tolist()\n",
    "all_paths = left_paths + right_paths\n",
    "patient_targets = df['target'].dropna().tolist()\n",
    "all_targets = patient_targets + patient_targets  # Duplicate for left & right images\n",
    "\n",
    "# Filter out invalid paths\n",
    "valid_paths = [p for p in all_paths if os.path.isfile(p)]\n",
    "if len(valid_paths) == 0:\n",
    "    raise ValueError(\"No valid images found. Please check the dataset.\")\n",
    "\n",
    "# Map textual labels to integer indices\n",
    "label_to_index = {label: idx for idx, label in enumerate(labels_list)}\n",
    "target_indices = [label_to_index[label] for label in all_targets]\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train_paths, X_val_paths, y_train, y_val = train_test_split(\n",
    "    valid_paths, target_indices, test_size=0.2, random_state=42\n",
    ")\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes=8)\n",
    "y_val = to_categorical(y_val, num_classes=8)\n",
    "\n",
    "# --- Build tf.data Pipeline ---\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16  # Reduced batch size to reduce memory usage\n",
    "\n",
    "def process_image(file_path, label):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    try:\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "    except Exception as e:\n",
    "        tf.print(f\"Error decoding {file_path}: {e}\")\n",
    "        image = tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    return image, label\n",
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE+20, IMG_SIZE+20)\n",
    "    image = tf.image.random_crop(image, size=[IMG_SIZE, IMG_SIZE, 3])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    return image, label\n",
    "\n",
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "def create_dataset(file_paths, labels, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    ds = ds.map(process_image, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(normalize, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(SHUFFLE_BUFFER)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.cache().prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = create_dataset(X_train_paths, y_train, training=True)\n",
    "val_ds = create_dataset(X_val_paths, y_val, training=False)\n",
    "\n",
    "# Diagnostic: Check one batch\n",
    "start = time.time()\n",
    "for batch in train_ds.take(1):\n",
    "    images_batch, labels_batch = batch\n",
    "    print(\"One batch shape:\", images_batch.shape)\n",
    "end = time.time()\n",
    "print(\"Time taken for one batch:\", end - start, \"seconds\")\n",
    "\n",
    "# --- Build the Model ---\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "base_model.trainable = False  # Freeze base model initially\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# --- Training ---\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=50,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[early_stopping, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ocular_disease_resnet50_model.h5\")\n",
    "print(\"✅ Model saved as 'ocular_disease_resnet50_model.h5'\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "loss, val_accuracy = model.evaluate(val_ds, verbose=1)\n",
    "print(f\"✅ Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "# Predictions for metrics\n",
    "y_pred = model.predict(val_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(np.concatenate([y for _, y in val_ds], axis=0), axis=1)\n",
    "\n",
    "# Additional Metrics\n",
    "roc_score = roc_auc_score(np.concatenate([y for _, y in val_ds], axis=0), y_pred, multi_class='ovr')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred_classes, average='weighted')\n",
    "\n",
    "# Number of Parameters and Layers\n",
    "num_params = model.count_params()\n",
    "num_layers = len(model.layers)\n",
    "\n",
    "print(\"\\n--- Metrics for Comparative Study ---\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_score:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"Number of Parameters: {num_params}\")\n",
    "print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"Number of Layers: {num_layers}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred_classes))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=labels_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35dcb800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metrics saved to 'Model_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"Accuracy\": val_accuracy,\n",
    "    \"ROC Score\": roc_score,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1_score,\n",
    "    \"Number of Parameters\": num_params,\n",
    "    \"Training Time (seconds)\": training_time,\n",
    "    \"Number of Layers\": num_layers\n",
    "}\n",
    "\n",
    "# Save metrics to a CSV file\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(\"Model_metrics.csv\", index=False)\n",
    "print(\"✅ Metrics saved to 'Model_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d208426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary saved to 'model_summary_vgg16.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model  # Correctly import load_model\n",
    "\n",
    "# Load your model\n",
    "model = load_model(\"ocular_disease_resnet50_model.h5\")\n",
    "\n",
    "# Save model summary to a text file\n",
    "with open('model_summary_resnet50.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "print(\"Model summary saved to 'model_summary_vgg16.txt'.\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='o')\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', marker='o')\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('resnet50_eval.png')  # Save the training curves plot\n",
    "print(\"Training curves saved to 'resnet50_eval.png'.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5feb5b3-b164-4f4c-bc7e-351613c9d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a89ff2d-cf28-4420-a052-1802c5b23d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d8a8ea-df0c-4be9-9661-a407785ba9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow detected 1 GPU(s).\n",
      "❌ PyTorch did not detect any GPUs.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# TensorFlow GPU check\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"✅ TensorFlow detected {len(gpus)} GPU(s).\")\n",
    "else:\n",
    "    print(\"❌ TensorFlow did not detect any GPUs.\")\n",
    "\n",
    "# PyTorch GPU check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ PyTorch detected {torch.cuda.device_count()} GPU(s).\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"❌ PyTorch did not detect any GPUs.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f6dcc9-4328-456d-b6a0-9169ef40826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83457659-4608-45ad-ab8d-436e17ac9c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4dab160-158e-44ac-be61-c681a45f0c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available (TensorFlow): 1\n",
      "PyTorch GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available (TensorFlow):\", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import torch\n",
    "print(\"PyTorch GPU Available:\", torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
